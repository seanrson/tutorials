{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoders: From Probability Theory to Practice\n",
    "\n",
    "## 1. Probabilistic Foundations\n",
    "Let $(\\Omega, \\sigma, \\mathbb{P})$ be a probability space. Our data points $\\{x_i\\}_{i=1}^n$ are realizations of random variables $X_i: \\Omega \\to \\mathbb{R}^d$. The push-forward measure $P_X = \\mathbb{P} \\circ X^{-1}$ defines the distribution of our data.\n",
    "We aim to learn this distribution through a latent variable model. For each observation $x_i$, we posit the existence of an unobserved latent variable $z_i$ which is a realization of a random variable $Z_i: \\Omega \\to \\mathbb{R}^k$, with push-forward measure $P_Z = \\mathbb{P} \\circ Z^{-1}$ and density $p_Z$ with respect to Lebesgue measure (which exists by Radon-Nikodym since we assume $P_Z \\ll \\lambda$ where $\\lambda$ is Lebesgue measure).\n",
    "\n",
    "## 2. The Model Class\n",
    "Let $p_X$ be the true (unknown) density of $P_X$ with respect to Lebesgue measure (which exists by Radon-Nikodym since we assume $P_X \\ll \\lambda$ where $\\lambda$ is Lebesgue measure).\n",
    "We aim to approximate $p_X$ using a latent variable model. Specifically, we consider a family of joint densities $p_{X,Z}(\\cdot, \\cdot; \\theta)$ parameterized by $\\theta$, which induce marginal densities:\n",
    "$$p_\\theta(x_i) = \\int_{\\mathbb{R}^k} p_{X,Z}(x_i, z; \\theta) dz$$\n",
    "The maximum likelihood objective is then:\n",
    "$$\\hat{\\theta} = \\argmax_{\\theta} \\sum_{i=1}^n \\log p_\\theta(x_i)$$\n",
    "\n",
    "## 3. The Evidence Lower Bound (ELBO)\n",
    "The integral in our objective is typically intractable. We can derive a lower bound through Jensen's inequality. For any probability density $q$:\n",
    "$$\\log p(x_i) = \\log \\int_{\\mathbb{R}^k} p(x_i, z) dz = \\log \\int_{\\mathbb{R}^k} q(z) \\frac{p(x_i, z)}{q(z)} dz$$\n",
    "By Jensen's inequality and the concavity of log:\n",
    "$$\\log \\mathbb{E}_{Z \\sim q}\\left[\\frac{p(x_i, Z)}{q(Z)}\\right] \\geq \\mathbb{E}_{Z \\sim q}\\left[\\log \\frac{p(x_i, Z)}{q(Z)}\\right]$$\n",
    "This gives us our evidence lower bound (ELBO):\n",
    "$$\\log p(x_i) \\geq \\int_{\\mathbb{R}^k} q(z) \\log \\frac{p(x_i, z)}{q(z)} dz$$\n",
    "\n",
    "## 4. Factorization and Amortization\n",
    "We can decompose the joint density using the chain rule: $p(x_i, z) = p(x_i|z)p(z)$. This allows us to rewrite the ELBO:\n",
    "$$\\mathbb{E}_{Z \\sim q}[\\log p(x_i|Z)] - D_\\text{KL}(q|p_Z)$$\n",
    "where $p_Z$ is our prior density on the latent space.\n",
    "This form is computationally tractable because:\n",
    "\n",
    "The first term can be estimated with Monte Carlo sampling:\n",
    "$$\\mathbb{E}_{Z \\sim q}[\\log p(x_i|Z)] \\approx \\frac{1}{S}\\sum_{s=1}^S \\log p(x_i|z^{(s)})$$\n",
    "where $z^{(s)} \\sim q(z; g_\\phi(x_i))$. The KL divergence term has a closed form when using normal distributions.\n",
    "\n",
    "This is much more tractable than the original objective:\n",
    "$\\log \\int p(x_i|z)p(z)dz$\n",
    "which would require integrating over all $z$ just to evaluate once.\n",
    "For additional computational efficiency, we make two key assumptions:\n",
    "\n",
    "$\\textbf{Mean-Field Assumption}$: The joint distribution of latent variables factors across data points:\n",
    "$$q(z_1, ..., z_n) = \\prod_{i=1}^n q_i(z_i)$$\n",
    "$\\textbf{Amortized Inference}$: Instead of learning separate variational distributions for each data point, we learn a mapping $g_\\phi: \\mathbb{R}^d \\to \\mathcal{P}(\\mathbb{R}^k)$ where $\\mathcal{P}(\\mathbb{R}^k)$ represents the space of probability measures on $\\mathbb{R}^k$:\n",
    "$$q_i(z_i) = q(z_i; g_\\phi(x_i))$$\n",
    "\n",
    "## 5. Parameterization and Computation\n",
    "We typically choose:\n",
    "\n",
    "Prior: $p(z) = \\mathcal{N}(0, I)$\n",
    "\n",
    "Variational family: $q(z; g_\\phi(x)) = \\mathcal{N}(\\mu_\\phi(x), \\text{diag}(\\sigma^2_\\phi(x)))$\n",
    "\n",
    "Likelihood: $p(x|z) = f_\\theta(z)$ for some neural network $f_\\theta$\n",
    "\n",
    "This gives us the final optimization objective:\n",
    "$$\\max_{\\theta, \\phi} \\sum_{i=1}^n \\left[\\mathbb{E}_{z \\sim q(z; g\\phi(x_i))} \\log p(x_i|z; f_\\theta) - D_\\text{KL}(q(z; g_\\phi(x_i))|p(z))\\right]$$\n",
    "The KL divergence term has a closed form for normal distributions. The expectation term requires Monte Carlo estimation, which is made differentiable through the reparameterization trick:\n",
    "Instead of sampling directly from $q(z; g_\\phi(x_i))$, we sample:\n",
    "$$\\epsilon \\sim \\mathcal{N}(0, I)$$\n",
    "$$z = \\mu_\\phi(x_i) + \\sigma_\\phi(x_i) \\odot \\epsilon$$\n",
    "This makes the sampling process differentiable with respect to the parameters $\\phi$.\n",
    "\n",
    "## Notes:\n",
    "This is the standard VI setup with local latent variables, not a fully Bayesian treatment with global latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m random, grad, jit, vmap\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jax'"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, grad, jit, vmap\n",
    "import tensorflow_datasets as tfds\n",
    "import optax\n",
    "\n",
    "def get_mnist():\n",
    "    \"\"\"Load and normalize MNIST data.\"\"\"\n",
    "    ds = tfds.load('mnist', split='train', as_supervised=True)\n",
    "    ds = ds.map(lambda x, y: (tf.cast(x, tf.float32) / 255., y))\n",
    "    return ds.batch(32).prefetch(1)\n",
    "\n",
    "def init_network_params(key, input_dim, hidden_dim, latent_dim):\n",
    "    \"\"\"Initialize neural network parameters.\"\"\"\n",
    "    # Encoder: input_dim -> hidden_dim -> latent_dim*2 (mean and logvar)\n",
    "    key1, key2, key3, key4 = random.split(key, 4)\n",
    "    \n",
    "    encoder_params = {\n",
    "        'h1': {\n",
    "            'w': random.normal(key1, (input_dim, hidden_dim)) / jnp.sqrt(input_dim),\n",
    "            'b': jnp.zeros(hidden_dim)\n",
    "        },\n",
    "        'h2': {\n",
    "            'w': random.normal(key2, (hidden_dim, latent_dim * 2)) / jnp.sqrt(hidden_dim),\n",
    "            'b': jnp.zeros(latent_dim * 2)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Decoder: latent_dim -> hidden_dim -> input_dim\n",
    "    decoder_params = {\n",
    "        'h1': {\n",
    "            'w': random.normal(key3, (latent_dim, hidden_dim)) / jnp.sqrt(latent_dim),\n",
    "            'b': jnp.zeros(hidden_dim)\n",
    "        },\n",
    "        'h2': {\n",
    "            'w': random.normal(key4, (hidden_dim, input_dim)) / jnp.sqrt(hidden_dim),\n",
    "            'b': jnp.zeros(input_dim)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return {'encoder': encoder_params, 'decoder': decoder_params}\n",
    "\n",
    "def encoder(params, x):\n",
    "    \"\"\"Encoder network mapping x to mean and logvar of q(z|x).\"\"\"\n",
    "    # First layer with tanh activation\n",
    "    h = jnp.tanh(x @ params['h1']['w'] + params['h1']['b'])\n",
    "    # Output layer with no activation (mean and logvar)\n",
    "    h = h @ params['h2']['w'] + params['h2']['b']\n",
    "    # Split into mean and logvar\n",
    "    mean, logvar = jnp.split(h, 2, axis=-1)\n",
    "    return mean, logvar\n",
    "\n",
    "def reparameterize(key, mean, logvar):\n",
    "    \"\"\"Reparameterization trick: z = mean + std * epsilon.\"\"\"\n",
    "    eps = random.normal(key, mean.shape)\n",
    "    return mean + jnp.exp(0.5 * logvar) * eps\n",
    "\n",
    "def decoder(params, z):\n",
    "    \"\"\"Decoder network mapping z to reconstruction.\"\"\"\n",
    "    # First layer with tanh activation\n",
    "    h = jnp.tanh(z @ params['h1']['w'] + params['h1']['b'])\n",
    "    # Output layer with sigmoid for pixel values\n",
    "    x_recon = jax.nn.sigmoid(h @ params['h2']['w'] + params['h2']['b'])\n",
    "    return x_recon\n",
    "\n",
    "def vae_loss(params, key, batch):\n",
    "    \"\"\"Compute ELBO loss for a batch.\"\"\"\n",
    "    # Encode\n",
    "    mean, logvar = encoder(params['encoder'], batch)\n",
    "    \n",
    "    # Sample z using reparameterization trick\n",
    "    z = reparameterize(key, mean, logvar)\n",
    "    \n",
    "    # Decode\n",
    "    x_recon = decoder(params['decoder'], z)\n",
    "    \n",
    "    # Reconstruction loss (binary cross entropy)\n",
    "    recon_loss = -jnp.sum(\n",
    "        batch * jnp.log(x_recon + 1e-8) + \n",
    "        (1 - batch) * jnp.log(1 - x_recon + 1e-8)\n",
    "    )\n",
    "    \n",
    "    # KL divergence (analytical for normal distributions)\n",
    "    kl_loss = -0.5 * jnp.sum(\n",
    "        1 + logvar - jnp.square(mean) - jnp.exp(logvar)\n",
    "    )\n",
    "    \n",
    "    return recon_loss + kl_loss\n",
    "\n",
    "# JIT compile for faster execution\n",
    "@jit\n",
    "def train_step(params, opt_state, key, batch):\n",
    "    \"\"\"Single training step.\"\"\"\n",
    "    loss_val, grads = jax.value_and_grad(vae_loss)(params, key, batch)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss_val\n",
    "\n",
    "# Training setup\n",
    "batch_size = 32\n",
    "input_dim = 784  # flattened MNIST\n",
    "hidden_dim = 512\n",
    "latent_dim = 2\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Initialize parameters and optimizer\n",
    "key = random.PRNGKey(0)\n",
    "params = init_network_params(key, input_dim, hidden_dim, latent_dim)\n",
    "optimizer = optax.adam(learning_rate)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "# Training loop\n",
    "def train_epoch(params, opt_state, key, train_ds):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    for batch, _ in train_ds:\n",
    "        key, subkey = random.split(key)\n",
    "        batch = jnp.reshape(batch, (batch_size, -1))\n",
    "        params, opt_state, loss = train_step(params, opt_state, subkey, batch)\n",
    "    return params, opt_state\n",
    "\n",
    "# To use:\n",
    "# train_ds = get_mnist()\n",
    "# for epoch in range(num_epochs):\n",
    "#     key, subkey = random.split(key)\n",
    "#     params, opt_state = train_epoch(params, opt_state, subkey, train_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
